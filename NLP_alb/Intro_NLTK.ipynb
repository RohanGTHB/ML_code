{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Import NLTKs corpora, books etc."},{"metadata":{},"cell_type":"markdown","source":"!python -m nltk.downloader all"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nimport numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explore \ndir(nltk.corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explore\nprint(nltk.corpus.__class__)\nnltk.corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.corpus.gutenberg.fileids()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import book\n\nprint(dir(nltk.book))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nltk.book conatins 9 texts, can be seen above ::\nfrom nltk.book import text4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(text4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(text4[:10])\nprint(np.shape(text4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(text4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of unique words for words with wordlength > 5 in text4 doc\n\nlen(set([word.lower() for word in text4 if len(word) >= 5])) # Total unique : 9913","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"' '.join(text4[:200])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Words in context\n\nSearch word in text, diasplay the results together with the context:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"text4.concordance?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#text4.concordance(\"people\")\ntext4.concordance(\"citizen\",width=30, lines=10)  #This isn't case sensitive","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What other words appear in a similar range of contexts? "},{"metadata":{"trusted":true},"cell_type":"code","source":"text4.similar?","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"text4.similar(\"citizen\", num=5) #This isn't case sensitive","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Examine just the contexts that are shared by two or more words:"},{"metadata":{"trusted":true},"cell_type":"code","source":"text4.common_contexts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text4.common_contexts([\"war\", \"democracy\",'peace'], num=5) # can be used for 2 or more words","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Location of a word in the text: how many spaces from the beginning does it appear? \n\nThis positional information can be displayed using a dispersion plot. \n\nYou need NumPy and Matplotlib. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start pylab inline mode, so figures will appear in the notebook\n%pylab inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\n# ?Counter\nc = Counter(text4)\n\nprint(c.most_common(4))\n\nprint(sum(c.values()))\n\nprint(c['vote'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\n\n# Dispersion plot \nfrom nltk.draw.dispersion import dispersion_plot\ndispersion_plot(text4, [\"citizens\", \"democracy\", \"freedom\", \"war\", \"America\", \"vote\",\"the\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Counting\nThe length of a text from start to finish, in terms of the words and punctuation symbols that appear. All tokens. \n\n\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(text4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Count how often a word occurs in a text:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"text4.count(\"war\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How many distinct words does the book of Genesis contain? \nThe vocabulary of a text is just the set of tokens that it uses. "},{"metadata":{"trusted":true},"cell_type":"code","source":"len(set(text4)) #types\n\n# Each word used on average x times. Richness of the text. \nlen(text4) / len(set(text4)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Define functions: \n\nWhat do you think they do? \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lexical_diversity(text):\n    \"\"\"\n    \"\"\"\n    return len(set(text))/float(len(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def percentage(count, total):\n    return count/float(total)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then use the defined functions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import division #to get precise (float) division in Python 2.x. In Python 3.0 you get it automatically. \nlexical_diversity(text4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"percentage(text4.count('the'), len(text4)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Simple statistics\n\nCounting Words Appearing in a Text (a frequency distribution). \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Freqdist is doing pretty much the same thing which counter from collections is doing\n\nfrom nltk import FreqDist\nfdist1 = FreqDist(text4)\nfdist1.items()\n\n# ordered abstraction of dictionary\nsorted(fdist1.items(), key=lambda kv: kv[1],reverse= True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(12, 6))\n# plot the top 20 tokens\nfdist1.plot(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hmmm..this is a good start...but we don't care about words like \"for\" and \"and\"\nfrom nltk.corpus import stopwords\n\n# let's use NLTK's built-in list of \"stopwords\"\nstoplist = stopwords.words('english')\nprint(stoplist)\n\nnp.shape(stoplist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Frequency plots after removing stop words\ntokens = [token for token in text4 if token not in stoplist]\n\nfrequencyDistribution = nltk.FreqDist(tokens)\nprint(frequencyDistribution.freq)\n\nplt.figure(figsize=(12, 6))\n# plot the top 20 tokens\nfrequencyDistribution.plot(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Frequency plots after removing single letter words\n\n# We don't want these symbols\nsymbols = {\"``\", \"''\", \":\", 'The', '--', ' ', ' - ', 'It', 'We'}\n\ntokens = [token for token in tokens if len(token)>1 and token not in symbols]\n\nfrequencyDistribution = nltk.FreqDist(tokens)\nprint(frequencyDistribution.freq)\n\nplt.figure(figsize=(12, 6))\n# plot the top 20 tokens\nfrequencyDistribution.plot(20)\n# MUCH BETTER!!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fdist1 is a collection of all the words existing \nvocabulary1 = fdist1.keys() # list of all the distinct types in the text\nvocabulary1 # look at first 3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- words that occur only once, called hapaxes: "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.shape(fdist1.hapaxes()))\nprint(fdist1.hapaxes())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" \n- words that meet a condition, are long for example\n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"V = set(text4)\nlong_words = [w for w in V if len(w) > 10]\nprint(sorted(long_words))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- words that characterize a text (are relatively long, and occur frequently)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Words with length > 12 and frequency > 7\n\nfdist = FreqDist(text4)\nsorted([w for w in set(text4) if len(w) > 12 and fdist[w] > 7])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.shape(tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Note: You need to install wordcloud using pip\n# Simple WordCloud\nfrom os import path\nfrom scipy.misc import imread\nimport matplotlib.pyplot as plt\nimport random\n\nfrom wordcloud import WordCloud, STOPWORDS      #need to install wordcloud package\n\n#text = 'all your base are belong to us all of your base base base'\nwordcloud = WordCloud(width = 1000, height = 500).generate(' '.join(tokens))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Conditional frequency distributions\n\nWorking with the inaugural corpus:\n\n\n\n\n\n\n\n\n\n\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import inaugural\n\ninaugural.fileids()[:2]\n\nprint([fileid for fileid in inaugural.fileids()][-10:] )\n# Get the first 4 characters of the file IDs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How are the words \"America\" and \"citizen\" are used over time?"},{"metadata":{"trusted":true},"cell_type":"code","source":"cfd = nltk.ConditionalFreqDist(\n    (target, fileid[:4])\n    \n    \n    for fileid in inaugural.fileids()\n    for w in inaugural.words(fileid)\n    for target in ['america', 'war']\n    if w.lower().startswith(target))\ncfd.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Working with the news database corpus:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import brown\n\nlen(set(inaugural.words())) # this is same as text 4\n\n# ?brown.words","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"news_words = brown.words(categories=\"news\") \nprint(news_words) # get the first words in the corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(news_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq = nltk.FreqDist(news_words)\nfreq.plot(30) # frequency of most commonly used words in the corpus","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How are different verbs used in different news genres? "},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import FreqDist\n\nverbs = [\"should\", \"may\", \"can\"]\ngenres = [\"news\", \"government\", \"romance\"]\n\nfor g in genres:\n    words=brown.words(categories=g)\n    freq=FreqDist([w.lower() for w in words if w.lower() in verbs])\n    print(g, freq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq # this is for romance -->  last iteration","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Stopwords\n\nWhat percentage of the words in a corpus are NOT stopwords? \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nlen(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def content_fraction(text):\n    \"\"\"\n    \"\"\"\n    stopwords = nltk.corpus.stopwords.words('english')\n    content = [w for w in text if w.lower() not in stopwords]\n    return len(content) / len(text)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This contains 52.23% of non-stop words\n\nprint(content_fraction(nltk.corpus.inaugural.words()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This contains 59.09% of non-stop words\n\ncontent_fraction(nltk.corpus.brown.words())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing and accessing your own text"},{"metadata":{},"cell_type":"markdown","source":"Useful libraries: "},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk, re, pprint\nimport requests\nimport urllib\n\n# urllib.request.urlopen(link)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### User input\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"s = input(\"Enter some text: \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Online articles\nGetting text out of HTML is a sufficiently common task that NLTK provides a helper function nltk.clean_html(), which takes an HTML string and returns raw text."},{"metadata":{"trusted":true},"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"url = \"http://www.bbc.co.uk/news/education-24367153\"\nhtml = requests.get(url).text\nsoup = BeautifulSoup(html, 'html.parser')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"raw = BeautifulSoup.get_text(soup)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"raw","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#raw = nltk.clean_html(html)\ntokens = nltk.word_tokenize(raw)\ntokens[:15]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text similarity"},{"metadata":{},"cell_type":"markdown","source":"We can use both NLTK and scikit-learn for this. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculate tf-idf:"},{"metadata":{"trusted":true},"cell_type":"code","source":"vect = TfidfVectorizer(min_df=1)\ntfidf = vect.fit_transform([\"New Year's Eve in New York\",\n                            \"New Year's Eve in London\",\n                            \"York is closer to London than to New York\",\n                            \"London is closer to Bucharest than to New York\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculate cosine similarity:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# vect.vocabulary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cosine=(tfidf * tfidf.T).A\nprint(cosine)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Trained classification with NLTK"},{"metadata":{},"cell_type":"markdown","source":"#### Names-gender identification example"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import names\nimport random","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Select relevant fearures. Here, last letter of name. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def gender_features(word):\n    return {'last_letter': word[-1]}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What is the feature for the name Shrek? \n\ngender_features('Shrek')\n\nWhat is the feature for your own name? "},{"metadata":{"trusted":true},"cell_type":"code","source":"# names.words('names.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gender_features('iulia')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train and test data: "},{"metadata":{"trusted":true},"cell_type":"code","source":"names = ([(name, 'male') for name in names.words('male.txt')] +\n          [(name, 'female') for name in names.words('female.txt')])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Arrange data randomly and extract features"},{"metadata":{"trusted":true},"cell_type":"code","source":"random.shuffle(names)\nfeaturesets = [(gender_features(n), g) for (n,g) in names]\nfrom nltk.classify import apply_features # use apply if you're working with large corpora","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Divide data into training and test sets:"},{"metadata":{"trusted":true},"cell_type":"code","source":"names[:10] # list containing (name,gender)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = apply_features(gender_features, names[500:1000])\ntest_set = apply_features(gender_features, names[:500])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use a Naive Bayes Classifier:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set[:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_set contins text column, nltk.NaiveBayesClassifier works on that aswell\nclassifier = nltk.NaiveBayesClassifier.train(train_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Classify the test set and evaluate performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(nltk.classify.accuracy(classifier, test_set))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What are the most informative features?"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.show_most_informative_features(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.classify(gender_features('iulia'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.classify(gender_features('cioroianu'))","execution_count":null,"outputs":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}