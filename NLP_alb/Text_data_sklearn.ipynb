{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Working with Text Data and Naive Bayes in scikit-learn"},{"metadata":{},"cell_type":"markdown","source":"## Agenda\n\n**Working with text data**\n\n- Representing text as data\n- Reading SMS data\n- Vectorizing SMS data\n- Examining the tokens and their counts\n- Bonus: Calculating the \"spamminess\" of each token\n\n**Naive Bayes classification**\n\n- Building a Naive Bayes model\n- Comparing Naive Bayes with logistic regression"},{"metadata":{},"cell_type":"markdown","source":"## Part 1: Representing text as data\n\nFrom the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n\n> Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect **numerical feature vectors with a fixed size** rather than the **raw text documents with variable length**.\n\nWe will use [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to \"convert text into a matrix of token counts\":"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# start with a simple example\nsimple_train = ['call you tonight', 'Call me a cab', 'please call me... PLEASE!']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"simple_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# learn the 'vocabulary' of the training data\nvect = CountVectorizer()\nvect.fit(simple_train)\n\nvect.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"- binary = True --> flag for absence or presence of a word in a TDM\n- ngram_"},{"metadata":{"trusted":true},"cell_type":"code","source":"vect\n\n# 'ngram_range' : The size of n_gram to make\n# binary : tells you only if the word is present, doesnot gives the frequency\n# stop_words : to specify incase stop words are not required","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CountVectorizer?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform training data into a 'document-term matrix'\nsimple_train_dtm = vect.transform(simple_train)\nsimple_train_dtm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the sparse matrix\n\n\n# This is a sparse matrix and returns [(row_i,col_i) freq if freq > 0]\nprint(simple_train_dtm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert sparse matrix to a dense matrix\nprint(simple_train_dtm.toarray().__class__)\nprint(simple_train_dtm.toarray(),'\\n')\n\nprint(simple_train_dtm.todense().__class__)\nprint(simple_train_dtm.todense())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# examine the vocabulary and document-term matrix together\nimport pandas as pd\npd.DataFrame(simple_train_dtm.toarray(), columns=vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n\n> In this scheme, features and samples are defined as follows:\n\n> - Each individual **token** occurrence frequency (normalized or not) is treated as a **feature**.\n> - The vector of all the token frequencies for a given document is considered a multivariate **sample**.\n\n> A **corpus of documents** can thus be represented by a matrix with **one row per document** and **one column per token** (e.g. word) occurring in the corpus.\n\n> We call **vectorization** the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the **Bag of Words** or \"Bag of n-grams\" representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document."},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform testing data into a document-term matrix (using existing vocabulary)\nsimple_test = [\"please don't call me\"]\nsimple_test_dtm = vect.transform(simple_test)\nsimple_test_dtm.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# examine the vocabulary and document-term matrix together\npd.DataFrame(simple_test_dtm.toarray(), columns=vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Summary:**\n\n- `vect.fit(train)` learns the vocabulary of the training data\n- `vect.transform(train)` uses the fitted vocabulary to build a document-term matrix from the training data\n- `vect.transform(test)` uses the fitted vocabulary to build a document-term matrix from the testing data (and ignores tokens it hasn't seen before)"},{"metadata":{},"cell_type":"markdown","source":"## Part 2: Reading SMS data"},{"metadata":{"trusted":true},"cell_type":"code","source":"sms = pd.read_csv('../input/sms.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sms.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sms.label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert label to a numeric variable\nsms['label'] = sms.label.map({'ham':0, 'spam':1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define X and y\nX = sms.message\ny = sms.label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)\nprint(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 3: Vectorizing SMS data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiate the vectorizer\nvect = CountVectorizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# learn training data vocabulary, then create document-term matrix\nvect.fit(X_train)\nX_train_dtm = vect.transform(X_train)\nX_train_dtm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# alternative: combine fit and transform into a single step\nX_train_dtm = vect.fit_transform(X_train)\nX_train_dtm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform testing data (using fitted vocabulary) into a document-term matrix\nX_test_dtm = vect.transform(X_test)\nX_test_dtm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 4: Examining the tokens and their counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"# store token names\nX_train_tokens = vect.get_feature_names()\n\nstring = list(filter(lambda x:x.isalpha(),X_train_tokens)) # filtering only string tokens\n\nnumeric = list(filter(lambda x:x.isnumeric(),X_train_tokens)) # filtering only numeric tokens\n\nstring[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view X_train_dtm as a dense matrix\nX_train_dtm.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# count how many times EACH token appears across ALL messages in X_train_dtm\n\n# sum for each column(vector)\nimport numpy as np\nX_train_counts = np.sum(X_train_dtm.toarray(), axis=0)\nX_train_counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_counts.shape  # Shape here is 7456 coz we are only learning only from x_train\n                      # whereas, this shape would change to 8713 vectors incase if we are learning(fitting) all values from the whole sms dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a DataFrame of tokens with their counts - X_train \n\npd.DataFrame({'token': X_train_tokens, 'count':X_train_counts}).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bonus: Calculating the \"spamminess\" of each token\n\n- This is for the whole dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create separate DataFrames for ham and spam\n\nsms_ham = sms[sms.label==0]\nsms_spam = sms[sms.label==1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The model would learn the vectors(when we call fit) on the whole sms dataset\n- After that when we call tranform(for ham and spam separately), the model would keep all the vectors but will calc the frequency separately for hama and spam"},{"metadata":{"trusted":true},"cell_type":"code","source":"# learn the vocabulary of ALL messages and save it\nvect.fit(sms.message)\nall_tokens = vect.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create document-term matrices for ham and spam\nham_dtm = vect.transform(sms_ham.message)\nspam_dtm = vect.transform(sms_spam.message)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# count how many times EACH token appears across ALL ham messages\nham_counts = np.sum(ham_dtm.toarray(), axis=0)\n\nprint(ham_counts.shape)\nham_counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# count how many times EACH token appears across ALL spam messages\nspam_counts = np.sum(spam_dtm.toarray(), axis=0)\n\nprint(spam_counts.shape)\nspam_counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a DataFrame of tokens with their separate ham and spam counts\n\n# This dataframe is made from 'tokens' from vector.fit(sms)\n# and then sms.fit_transform(ham and spam) \n\ntoken_counts = pd.DataFrame({'token':all_tokens, 'ham':ham_counts, 'spam':spam_counts})\ntoken_counts.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add one to ham and spam counts to avoid dividing by zero (in the step that follows)\ntoken_counts['ham'] = token_counts.ham + 1\ntoken_counts['spam'] = token_counts.spam + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate ratio of spam-to-ham for each token\ntoken_counts['spam_ratio'] = token_counts.spam / token_counts.ham\ntoken_counts.sort_values('spam_ratio').head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 5: Building a Naive Bayes model\n\nWe will use [Multinomial Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html):\n\n> The multinomial Naive Bayes classifier is suitable for classification with **discrete features** (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We have 4179 messages and 7456 vectors\n\nprint(X_train_dtm.toarray().shape) # Here the vector space is fromed from training set and transform set is also from training set\nprint(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_dtm.toarray()[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train a Naive Bayes model using X_train_dtm\nfrom sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\nnb.fit(X_train_dtm, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make class predictions for X_test_dtm\n\nprint(X_test_dtm.shape)\ny_pred_class = nb.predict(X_test_dtm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate accuracy of class predictions\nfrom sklearn import metrics\nprint(metrics.accuracy_score(y_test, y_pred_class))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix\nprint(metrics.confusion_matrix(y_test, y_pred_class))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict (poorly calibrated) probabilities\ny_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\ny_pred_prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate AUC\nprint(metrics.roc_auc_score(y_test, y_pred_prob))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print message text for the false positives\nX_test[y_test < y_pred_class]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print message text for the false negatives\nX_test[y_test > y_pred_class]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# what do you notice about the false negatives?\nX_test[3132]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 6: Comparing Naive Bayes with logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import/instantiate/fit\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(C=1e9)\nlogreg.fit(X_train_dtm, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# class predictions and predicted probabilities\ny_pred_class = logreg.predict(X_test_dtm)\ny_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate accuracy and AUC\nprint(metrics.accuracy_score(y_test, y_pred_class))\nprint(metrics.roc_auc_score(y_test, y_pred_prob))","execution_count":null,"outputs":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}