{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Text Mining using Natural Language Processing (NLP)"},{"metadata":{},"cell_type":"markdown","source":"## Introduction"},{"metadata":{},"cell_type":"markdown","source":"### What is NLP?\n\n- Using computers to process (analyze, understand, generate) natural human languages\n- Most knowledge created by humans is unstructured text, and we need a way to make sense of it\n- Build probabilistic model using data about a language"},{"metadata":{},"cell_type":"markdown","source":"### Important Packages Related to Textmining\n- **textmining1.0:** contains a variety of useful functions for text mining in Python.\n- **NLTK:** This package can be extremely useful because you have easy access to over 50 corpora and lexical resources\n- **Tweepy:** to mine Twitter data\n- **scrappy:**  extract the data you need from websites\n- **urllib2:** a package for opening URLs\n- **requests:** library for grabbing data from the internet\n- **Beautifulsoup:** library for parsing HTML data\n- **re:**  grep(), grepl(), regexpr(), gregexpr(), sub(), gsub(), and strsplit() are helpful functions\n- **wordcloud:** to visualize the wordcloud\n- **Textblob:** to used for text processing (nlp- lowel events)\n- **sklearn:** to used for preprocessing, modeling"},{"metadata":{},"cell_type":"markdown","source":"### What are some of the higher level task areas?\n\n- **Information retrieval**: Find relevant results and similar results\n    - [Google](https://www.google.com/)\n- **Information extraction**: Structured information from unstructured documents\n    - [Events from Gmail](https://support.google.com/calendar/answer/6084018?hl=en)\n- **Machine translation**: One language to another\n    - [Google Translate](https://translate.google.com/)\n- **Text simplification**: Preserve the meaning of text, but simplify the grammar and vocabulary\n    - [Rewordify](https://rewordify.com/)\n    - [Simple English Wikipedia](https://simple.wikipedia.org/wiki/Main_Page)\n- **Predictive text input**: Faster or easier typing\n    - [My application](https://justmarkham.shinyapps.io/textprediction/)\n    - [A much better application](https://farsite.shinyapps.io/swiftkey-cap/)\n- **Sentiment analysis**: Attitude of speaker\n    - [Hater News](http://haternews.herokuapp.com/)\n- **Automatic summarization**: Extractive or abstractive summarization\n    - [autotldr](https://www.reddit.com/r/technology/comments/35brc8/21_million_people_still_use_aol_dialup/cr2zzj0)\n- **Natural Language Generation**: Generate text from data\n    - [How a computer describes a sports match](http://www.bbc.com/news/technology-34204052)\n    - [Publishers withdraw more than 120 gibberish papers](http://www.nature.com/news/publishers-withdraw-more-than-120-gibberish-papers-1.14763)\n- **Speech recognition and generation**: Speech-to-text, text-to-speech\n    - [Google's Web Speech API demo](https://www.google.com/intl/en/chrome/demos/speech.html)\n    - [Vocalware Text-to-Speech demo](https://www.vocalware.com/index/demo)\n- **Question answering**: Determine the intent of the question, match query with knowledge base, evaluate hypotheses\n    - [How did supercomputer Watson beat Jeopardy champion Ken Jennings?](http://blog.ted.com/how-did-supercomputer-watson-beat-jeopardy-champion-ken-jennings-experts-discuss/)\n    - [IBM's Watson Trivia Challenge](http://www.nytimes.com/interactive/2010/06/16/magazine/watson-trivia-game.html)\n    - [The AI Behind Watson](http://www.aaai.org/Magazine/Watson/watson.php)"},{"metadata":{},"cell_type":"markdown","source":"### Data Processing - What are some of the lower level components?\n\n- **Tokenization**: breaking text into tokens (words, sentences, n-grams)\n- **Stopword removal**: a/an/the\n- **Stemming and lemmatization**: root word\n- **TF-IDF**: word importance\n- **Part-of-speech tagging**: noun/verb/adjective\n- **Named entity recognition**: person/organization/location\n- **Spelling correction**: \"New Yrok City\"\n- **Word sense disambiguation**: \"buy a mouse\"\n- **Segmentation**: \"New York City subway\"\n- **Language detection**: \"translate this page\"\n- **Machine learning**"},{"metadata":{},"cell_type":"markdown","source":"### Why is NLP hard?\n\n- **Ambiguity**:\n    - Hospitals are Sued by 7 Foot Doctors\n    - Juvenile Court to Try Shooting Defendant\n    - Local High School Dropouts Cut in Half\n- **Non-standard English**: text messages\n- **Idioms**: \"throw in the towel\"\n- **Newly coined words**: \"retweet\"\n- **Tricky entity names**: \"Where is A Bug's Life playing?\"\n- **World knowledge**: \"Mary and Sue are sisters\", \"Mary and Sue are mothers\"\n\nNLP requires an understanding of the **language** and the **world**."},{"metadata":{},"cell_type":"markdown","source":"## Text Classification"},{"metadata":{},"cell_type":"markdown","source":"#### Feature Engineering\n##### TF-IDF Vectors as features\n- TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n- IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n\n- TF-IDF Vectors can be generated at different levels of input tokens (words, characters, n-grams)\n    - a. Word Level TF-IDF : Matrix representing tf-idf scores of every term in different documents\n    - b. N-gram Level TF-IDF : N-grams are the combination of N terms together. This Matrix representing tf-idf scores of N-grams\n    - c. Character Level TF-IDF : Matrix representing tf-idf scores of character level n-grams in the corpus\n\n##### Text / NLP based features\n- Word Count of the documents – total number of words in the documents\n- Character Count of the documents – total number of characters in the documents\n- Average Word Density of the documents – average length of the words used in the documents\n- Puncutation Count in the Complete Essay – total number of punctuation marks in the documents\n- Upper Case Count in the Complete Essay – total number of upper count words in the documents\n- Title Word Count in the Complete Essay – total number of proper case (title) words in the documents\n- Frequency distribution of Part of Speech Tags:\n    - Noun Count\n    - Verb Count\n    - Adjective Count\n    - Adverb Count\n    - ronoun Count\n    "},{"metadata":{},"cell_type":"markdown","source":"### Model Building\n- Naive Bayes Classifier\n- Linear Classifier\n- Support Vector Machine\n- KNN\n- Bagging Models\n- Boosting Models\n- Shallow Neural Networks\n- Deep Neural Networks\n    - Convolutional Neural Network (CNN)\n    - Long Short Term Modelr (LSTM)\n    - Gated Recurrent Unit (GRU)\n    - Bidirectional RNN\n    - Recurrent Convolutional Neural Network (RCNN)\n    - Other Variants of Deep Neural Networks"},{"metadata":{},"cell_type":"markdown","source":"## Part 1: Reading in the Yelp Reviews"},{"metadata":{},"cell_type":"markdown","source":"- \"corpus\" = collection of documents\n- \"corpora\" = plural form of corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import required packages\n#basics\nimport pandas as pd \nimport numpy as np\n\n#misc\nimport gc\nimport time\nimport warnings\n\n#stats\n#from scipy.misc import imread\nfrom scipy import sparse\nimport scipy.stats as ss\n\n#viz\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec \nimport seaborn as sns\nfrom wordcloud import WordCloud ,STOPWORDS\nfrom PIL import Image\n#import matplotlib_venn as venn\n\n#nlp\nimport string\nimport re    #for regex\nimport nltk\nfrom nltk.corpus import stopwords\n\n#import spacy\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\n\n# Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import TweetTokenizer   \n\n\n#FeatureEngineering\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer, TfidfTransformer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_is_fitted\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, decomposition, ensemble\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\n\nimport  textblob\n#import xgboost\n#from keras.preprocessing import text, sequence\n#from keras import layers, models, optimizers\n\nfrom textblob import TextBlob\nfrom nltk.stem import PorterStemmer\nimport nltk\n#nltk.download('wordnet')\nfrom textblob import Word\n\n#settings\nstart_time=time.time()\ncolor = sns.color_palette()\nsns.set_style(\"dark\")\neng_stopwords = set(stopwords.words(\"english\"))\nwarnings.filterwarnings(\"ignore\")\n\nlem = WordNetLemmatizer()\ntokenizer=TweetTokenizer()\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read yelp.csv into a DataFrame\nyelp = pd.read_csv(r'../input/data-review/reviews.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yelp.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yelp=yelp[['review_id', 'stars', 'text', 'cool', 'useful', 'funny']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yelp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = yelp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Basic Exploratory Analysis"},{"metadata":{},"cell_type":"markdown","source":"![](http://)#### Note: It may take some time to process the function if the data is huge\n\n#### Here we are trying to create some processed columns "},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'].astype(str)\ndf['count_sent']=df[\"text\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n\n#Word count in each comment:\ndf['count_word']=df[\"text\"].apply(lambda x: len(str(x).split()))\n\n#Unique word count\ndf['count_unique_word']=df[\"text\"].apply(lambda x: len(set(str(x).split())))\n\n#Letter count\ndf['count_letters']=df[\"text\"].apply(lambda x: len(str(x)))\n\n#Word density\n\ndf['word_density'] = df['count_letters'] / (df['count_word']+1)\n\n#punctuation count\ndf[\"count_punctuations\"] =df[\"text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n#upper case words count\ndf[\"count_words_upper\"] = df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n#upper case words count\ndf[\"count_words_lower\"] = df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.islower()]))\n\n#title case words count\ndf[\"count_words_title\"] = df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n#Number of stopwords\ndf[\"count_stopwords\"] = df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n\n#Average length of the words\ndf[\"mean_word_len\"] = df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\n#Number of numeric\ndf['numeric'] = df['text'].apply(lambda x :len([x for x in x.split() if x.isdigit()]))\n\n#Number of alphanumeric\ndf['alphanumeric'] = df['text'].apply(lambda x :len([x for x in x.split() if x.isalnum()]))\n\n#Number of alphabetics\ndf['alphabetetics'] = df['text'].apply(lambda x :len([x for x in x.split() if x.isalpha()]))\n\n#Number of alphabetics\ndf['Spaces'] = df['text'].apply(lambda x :len([x for x in x.split() if x.isspace()]))\n\n#Number of Words ends with\ndf['words_ends_with_et'] = df['text'].apply(lambda x :len([x for x in x.lower().split() if x.endswith('et')]))\n\n#Number of Words ends with\ndf['words_start_with_no'] = df['text'].apply(lambda x :len([x for x in x.lower().split() if x.startswith('no')]))\n\n# Count the occurences of all words\ndf['wordcounts'] = df['text'].apply(lambda x :dict([ [t, x.split().count(t)] for t in set(x.split()) ]))\n\npos_family = {\n    'noun' : ['NN','NNS','NNP','NNPS'],\n    'pron' : ['PRP','PRP$','WP','WP$'],\n    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n    'adj' :  ['JJ','JJR','JJS'],\n    'adv' : ['RB','RBR','RBS','WRB']\n}\n\n# function to check and get the part of speech tag count of a words in a given sentence\ndef check_pos_tag(x, flag):\n    cnt = 0\n    try:\n        wiki = textblob.TextBlob(x)\n        for tup in wiki.tags:\n            ppo = list(tup)[1]\n            if ppo in pos_family[flag]:\n                cnt += 1\n    except:\n        pass\n    return cnt\n\ndf['noun_count'] = df['text'].apply(lambda x: check_pos_tag(x, 'noun'))\ndf['verb_count'] = df['text'].apply(lambda x: check_pos_tag(x, 'verb'))\ndf['adj_count']  = df['text'].apply(lambda x: check_pos_tag(x, 'adj'))\ndf['adv_count']  = df['text'].apply(lambda x: check_pos_tag(x, 'adv'))\ndf['pron_count'] = df['text'].apply(lambda x: check_pos_tag(x, 'pron')) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculating Sentiment analysis using Textblob module"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sentiment - ranges from -1,1 : \n# -ve sentiment value denotes negitive sentiment and similarly for + || 0 value indicates neutral","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sentiment'] = df[\"text\"].apply(lambda x: TextBlob(x).sentiment.polarity )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yelp.head(2) == df.head(2) ## both are the same, as earlier assigned ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yelp.stars.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split the data into train & Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a new DataFrame that only contains the 5-star and 1-star reviews\n#yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n\n# define X and y\nX = yelp.text\ny = yelp.stars\n\n# split the new DataFrame into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yelp.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yelp.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating user defined functions for clean the text and pre-process the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Abbrevations and Words correction\ndef clean_text(text):\n    text = text.lower()\n    text = text.strip()\n    text = re.sub(r' +', ' ', text)\n    text = re.sub(r\"[-()\\\"#/@;:{}`+=~|.!?,'0-9]\", \"\", text)  # removing the unneccessary symbols from the text\n    return(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop = set(nltk.corpus.stopwords.words('english'))\nprint(stop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\ndef pre_process(text):\n    #text = text.str.replace('/','')                           #Replacing the / with none\n    #text = text.apply(lambda x: re.sub(\"  \",\" \", x))          #Replacing double space with single space\n    #text = re.sub(r\"[-()\\\"#/@;:{}`+=~|.!?,']\", \"\", text)      #Replacing special character with none\n    #text = re.sub(r'[0-9]+', '', text)                        #Replacing numbers with none\n    #text = text.apply(lambda x: \" \".join(x.translate(str.maketrans('', '', string.punctuation)) for x in x.split() if x.isalpha()))\n    text = text.apply(lambda x: \" \".join(x for x in x.split() if x not in stop)) #Removing stop words\n    #text = text.apply(lambda x: str(TextBlob(x).correct()))                      #Correct spelling corrections\n    #text = text.apply(lambda x: \" \".join(PorterStemmer().stem(word) for word in x.split())) #Stemming using porter stemmer\n    #text = text.apply(lambda x: \" \".join(stemmer_func(word) for word in x.split()))        #Stemming\n    #text = text.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))   #lemmatization\n    #text = text.apply(lambda x: \" \".join(word for word, pos in pos_tag(x.split()) if pos not in ['NN','NNS','NNP','NNPS'])) #Removing nouns etc\n    return(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.apply(lambda x: clean_text(x))\nX_test = X_test.apply(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=pre_process(X_train)\nX_test=pre_process(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[:2]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"### Vectorization (Count, Tfidf, Hashing)\n        * Charter level\n        * Word level\n        * n-grams"},{"metadata":{"trusted":true},"cell_type":"code","source":"?CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train\ncount_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', #  token pattern means that we are specifying here that each word would be separateed by a space\n                             ngram_range=(1, 1 ), \n                             min_df=5, # minimum frequency that the word should come in the whole document(1 document = 1 review)\n                             # max_df = 500  # maximum frequency that the word should come in the whole document\n                             encoding='latin-1' , # generally this utf-8\n                             max_features=800) # Limiting the no. of features\n\nxtrain_count = count_vect.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain_count","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### View the document term metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtm=xtrain_count.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(count_vect.get_feature_names()[:100])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtm1=pd.DataFrame(dtm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtm1.columns=count_vect.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtm1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vectorization (count, tfidf) for both train & test\n\n- TfidfVectorizer is used on sentences, while TfidfTransformer is used on an existing count matrix, such as one returned by CountVectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train\ncount_vect = CountVectorizer(analyzer='word', \n                             token_pattern=r'\\w{1,}', \n                             ngram_range=(1, 1 ), \n                             min_df=5, \n                             encoding='latin-1' , \n                             max_features=800)\n\nxtrain_count = count_vect.fit_transform(X_train)\n\n\n# ---- tf-idf vector --------------------------------\n\ntfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(xtrain_count)\n\n#Test\n#count_vect = CountVectorizer()\nxtest_count = count_vect.transform(X_test)\n\n#tfidf_transformer = TfidfTransformer()\nX_test_tfidf = tfidf_transformer.transform(xtest_count)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtm2=pd.DataFrame(X_train_tfidf.toarray(), columns=count_vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is tf-idf matrix unlike the one shown earlier which is a DTM \n\ndtm2.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Term frequency(TF) = # word occurance in a doc / total words in the doc ----> calc at document level\n- IDF = # log(docs the word is in / total number of docs) ---->  calc at overall  level\n\n#### > TF-IDF : tf*idf"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.text.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ngram level tf-idf \ntfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1, 2), max_features=800)\ntfidf_vect_ngram.fit(df['text'])\nxtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\nxtest_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Analysing how many vectors have non_zero presence\n(pd.DataFrame(xtrain_tfidf_ngram.toarray()).sum() > 0).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gives you the counter(list) output\n\nimport itertools\n\nd = tfidf_vect_ngram.vocabulary_\nlist(itertools.islice(d.items(), 0, 4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# characters level tf-idf\ntfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(1,2), max_features=800)\ntfidf_vect_ngram_chars.fit(df['text'])\nxtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train) \nxtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"?TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"?decomposition.LatentDirichletAllocation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Topic Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_topics.shape)  # Here 10 : represents topics   # 7500 : represents comments ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_word.shape\n\nnp.sum(topic_word,axis =1)  ## wtf is this?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in topic_word:\n    print(x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we are taking the last 5 words from a corpus of vectors that is sorted wrt topic_word vector's first value/topic  \n\nnp.array(vocab)[np.argsort(topic_word[0])][:-(n_top_words+1):-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.array(vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train a LDA Model\n\nlda_model = decomposition.LatentDirichletAllocation(n_components=10, learning_method='batch', max_iter=20) # n_components :  no of top words sentences to show \n\nX_topics = lda_model.fit_transform(X_train_tfidf) # contains info @ row level ::: (row,n_components)\ntopic_word = lda_model.components_   #  ::: (n_components,columns/vectors) \nvocab = count_vect.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view the topic models\n\nn_top_words = 5 # no of top words to shoe in each sentence \n\ntopic_summaries = []\nfor i, topic_dist in enumerate(topic_word):\n    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n    topic_summaries.append(' '.join(topic_words))\n\ntopic_summaries","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Yelp data - Word clouds"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import FreqDist\n\na = 'I am rohan and i live in delhi'\n\nprint(a.count('rohan'))\n\nFreqDist(a.lower().split(' ')) # works on list object\n\n#\n#nltk.wordpunct_tokenize(yelp['text'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frequency_words_wo_stop= {}\nfor data in yelp['text']:\n    tokens = nltk.wordpunct_tokenize(data.lower())\n    for token in tokens:\n        if token.lower() not in stop:\n            if token in frequency_words_wo_stop:\n                count = frequency_words_wo_stop[token]\n                count = count + 1\n                frequency_words_wo_stop[token] = count\n            else:\n                frequency_words_wo_stop[token] = 1\n                \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frequency_words_wo_stop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud ,STOPWORDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(stopwords=[]).generate(' '.join(X_train.tolist()))\n\n%matplotlib inline\nfig = plt.figure(figsize=(10,3))\nplt.imshow(wordcloud)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create user defined function for train the models"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(classifier, feature_vector_train, label, feature_vector_valid,  valid_y, is_neural_net=False):\n    # fit the training dataset on the classifier\n    classifier.fit(feature_vector_train, label)\n    \n    # predict the labels on validation dataset\n    predictions = classifier.predict(feature_vector_valid)\n    \n    if is_neural_net:\n        predictions = predictions.argmax(axis=-1)\n    \n    return metrics.accuracy_score(predictions, valid_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building different models with different vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Naive Bayes\n# Naive Bayes on Count Vectors and TF-IDF\naccuracy_L1 = train_model(naive_bayes.MultinomialNB(), X_train_tfidf, y_train, X_test_tfidf, y_test)\nprint(\"NB  for L1, Count Vectors: \", accuracy_L1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Naive Bayes on Word Level TF IDF Vectors\naccuracy_L1 = train_model(naive_bayes.MultinomialNB(), xtrain_count, y_train, xtest_count, y_test)\nprint(\"NB  for L1, WordLevel TF-IDF: \", accuracy_L1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Naive Bayes on Ngram Level TF IDF Vectors\naccuracy_L1 = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, y_train, xtest_tfidf_ngram, y_test)\nprint(\"NB  for L1, N-Gram Vectors: \", accuracy_L1)\n\n\n\n# Naive Bayes on Character Level TF IDF Vectors\naccuracy_L1 = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, y_train, xtest_tfidf_ngram_chars, y_test)\nprint(\"NB for L1, CharLevel Vectors: \", accuracy_L1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic Regression\n# Logistic Regression on Count Vectors and TF-IDF\naccuracy_L1 = train_model(LogisticRegression(), X_train_tfidf, y_train, X_test_tfidf, y_test)\nprint(\"LR  for L1, Count Vectors: \", accuracy_L1)\n\n\n\n# Logistic Regression on Word Level TF IDF Vectors\naccuracy_L1 = train_model(LogisticRegression(), xtrain_count, y_train, xtest_count, y_test)\nprint(\"LR  for L1, WordLevel TF-IDF: \", accuracy_L1)\n\n\n\n# Logistic Regression on Ngram Level TF IDF Vectors\naccuracy_L1 = train_model(LogisticRegression(), xtrain_tfidf_ngram, y_train, xtest_tfidf_ngram, y_test)\nprint(\"LR  for L1, N-Gram Vectors: \", accuracy_L1)\n\n\n\n# Logistic Regression on Character Level TF IDF Vectors\naccuracy_L1 = train_model(LogisticRegression(), xtrain_tfidf_ngram_chars, y_train, xtest_tfidf_ngram_chars, y_test)\nprint(\"LR for L1, CharLevel Vectors: \", accuracy_L1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Linear SVC\n# Linear SVC on Count Vectors and TF-IDF\naccuracy_L1 = train_model(svm.LinearSVC(), X_train_tfidf, y_train, X_test_tfidf, y_test)\nprint(\"SVC  for L1, Count Vectors: \", accuracy_L1)\n\n\n\n# Linear SVC on Word Level TF IDF Vectors\naccuracy_L1 = train_model(svm.LinearSVC(), xtrain_count, y_train, xtest_count, y_test)\nprint(\"SVC  for L1, WordLevel TF-IDF: \", accuracy_L1)\n\n\n\n# Linear SVC on Ngram Level TF IDF Vectors\naccuracy_L1 = train_model(svm.LinearSVC(), xtrain_tfidf_ngram, y_train, xtest_tfidf_ngram, y_test)\nprint(\"SVC  for L1, N-Gram Vectors: \", accuracy_L1)\n\n\n\n# Linear SVC on Character Level TF IDF Vectors\naccuracy_L1 = train_model(svm.LinearSVC(), xtrain_tfidf_ngram_chars, y_train, xtest_tfidf_ngram_chars, y_test)\nprint(\"SVC for L1, CharLevel Vectors: \", accuracy_L1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding Features to a Document-Term Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a DataFrame that only contains the 5-star and 1-star reviews ::: Breaking the problem into binary to increase the accuracy\n\nyelp = yelp[(yelp.stars==5) | (yelp.stars==1)]\n\n# define X and y\nfeature_cols = ['text', 'cool', 'useful', 'funny']\nX = yelp[feature_cols]\ny = yelp.stars\n\n# split into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TfidfVectorizer?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use CountVectorizer with text column only\nvect = TfidfVectorizer(lowercase=True, stop_words='english', max_features=1000, min_df=5, ngram_range=(1, 2))\nX_train_dtm = vect.fit_transform(X_train.text)\nX_test_dtm = vect.transform(X_test.text)\nprint(X_train_dtm.shape)\nprint(X_test_dtm.shape)\n\n# shape of other four feature columns\nX_train.drop('text', axis=1).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use CountVectorizer with text column only\nvect = CountVectorizer()\nX_train_dtm = vect.fit_transform(X_train.text)\nX_test_dtm = vect.transform(X_test.text)\nprint(X_train_dtm.shape)\nprint(X_test_dtm.shape)\n\n# shape of other four feature columns\nX_train.drop('text', axis=1).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extra.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cast other feature columns to float and convert to a sparse matrix\nextra = sparse.csr_matrix(X_train.drop('text', axis=1).astype(float))\nextra.shape\n\n# combine sparse matrices\nX_train_dtm_extra = sparse.hstack((X_train_dtm, extra))\nX_train_dtm_extra.shape\n\n# repeat for testing set\nextra = sparse.csr_matrix(X_test.drop('text', axis=1).astype(float))\nX_test_dtm_extra = sparse.hstack((X_test_dtm, extra))\nX_test_dtm_extra.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use logistic regression with text column only\nlogreg = LogisticRegression(C=1e9)\nlogreg.fit(X_train_dtm, y_train)\ny_pred_class = logreg.predict(X_test_dtm)\nprint(metrics.accuracy_score(y_test, y_pred_class))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use logistic regression with all features\nlogreg = LogisticRegression(C=1e9)\nlogreg.fit(X_train_dtm_extra, y_train)\ny_pred_class = logreg.predict(X_test_dtm_extra)\nprint(metrics.accuracy_score(y_test, y_pred_class))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Introduction to additional TextBlob features"},{"metadata":{},"cell_type":"markdown","source":"TextBlob: \"Simplified Text Processing\""},{"metadata":{"trusted":true},"cell_type":"code","source":"print(yelp.text[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the first review\nreview = TextBlob(yelp.text[0])\n\nprint(yelp.text[1],'\\n -------------------------------------------')\n\n\n# subjectivity {0,1} || polarity {-1,1}\nprint(review.sentiment) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save it as a TextBlob object\nreview = TextBlob(yelp.text[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dir(review))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(review.ngrams(2)[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review.sentiment","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list the words\nreview.words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list the sentences : maybe separated by \".\"\nreview.sentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# some string methods are available\nreview.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review.ngrams(n=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Language correction, detection, translation etc...\n\n- Vocab of textblog is \"English\"\n- It does not inclusde words like 'gmail.com', so infering them would be difficult"},{"metadata":{"trusted":true},"cell_type":"code","source":"# spelling correction\nTextBlob('15 minuets late').correct()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s=\"this is bcz\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TextBlob(s).correct()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# spellcheck\nWord('parot').spellcheck()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# definitions\nWord('bank').define('v')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# language detection\nTextBlob('Hola amigos').detect_language()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Language Translation\na=' '.join(TextBlob('టీమిండియా మాజీ ఓపెనర్‌ గౌతమ్ గంభీర్ మంగళవారం క్రికెట్‌కు రిటైర్‌మెంట్ ప్రకటించారు. దానిపై స్పందించిన బాలీవుడ్ స్టార్‌ షారుక్‌ ఖాన్‌ ఓ ట్వీట్ చేశారు').translate(to='en').words)\n\nprint(a)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stemming and Lemmatization"},{"metadata":{},"cell_type":"markdown","source":"**Stemming:**\n\n- **What:** Reduce a word to its base/stem/root form\n- **Why:** Often makes sense to treat related words the same way\n- **Notes:**\n    - Uses a \"simple\" and fast rule-based approach\n    - Stemmed words are usually not shown to users (used for analysis/indexing)\n    - Some search engines treat words with the same stem as synonyms"},{"metadata":{"trusted":true},"cell_type":"code","source":"# initialize stemmer\nstemmer = nltk.stem.snowball.SnowballStemmer('english')\nstemmer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(review.words)\nnp.shape(review.words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stem each word\nprint([stemmer.stem(word) for word in review.words])\nprint(np.shape([stemmer.stem(word) for word in review.words]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lemmatization**\n\n- **What:** Derive the canonical form ('lemma') of a word\n- **Why:** Can be better than stemming\n- **Notes:** Uses a dictionary-based approach (slower than stemming)"},{"metadata":{"trusted":true},"cell_type":"code","source":"review.words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# assume every word is a noun\nprint([word.lemmatize() for word in review.words])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# assume every word is a verb\nprint([word.lemmatize(pos='v') for word in review.words])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}