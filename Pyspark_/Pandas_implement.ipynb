{"cells":[{"cell_type":"code","source":["import pyspark\n\nfrom pyspark.sql import SparkSession # importting spark context( basic builder element for Dataframe API)\n\nspark = SparkSession.builder.appName('basics').getOrCreate() "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd \n\nfrom pyspark.sql.types import *  # Necessary for creating schemas\nfrom pyspark.sql.functions import * # Importing PySpark functions\n\nfrom pyspark.sql.window import Window # for partitionby"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["df = spark.sql('select * from ginni_csv')\n\n#spark.read.csv('/FileStore/tables/fakefriends.csv')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["df.show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-----+------+\nName|sales|profit|\n+----+-----+------+\n  A1| 6805|    49|\n  A2| 6110|    28|\n+----+-----+------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["df.describe().show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+----+------------------+------------------+\nsummary|Name|             sales|            profit|\n+-------+----+------------------+------------------+\n  count|  25|                25|                25|\n   mean|null|            5368.4|             51.68|\n stddev|null|2159.6297329249137|24.489317398953094|\n    min|  A1|              1431|               100|\n    max|  A9|              9951|                98|\n+-------+----+------------------+------------------+\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["#### Creating lead/lag \n\n# df.select(lag('sales',1).over(Window.partitionBy('Name').orderBy('Name'))).show()\ndf.select(lag('sales',1).over(Window.orderBy('Name')).alias('lead_lag')).show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+\nlead_lag|\n+--------+\n    null|\n    6805|\n+--------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["#### Ranking within a group - 2nd highest element\n\ndf1 = df.unionAll(df)\n\nwindowval = (Window.partitionBy('Name').orderBy(desc('sales')))\n\ncounter = df1.select('Name',row_number().over(windowval).alias('row_count'))\ncounter.filter(col('row_count') == 2).show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---------+\nName|row_count|\n+----+---------+\n A23|        2|\n  A9|        2|\n+----+---------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["#### Adding index to a spark DF - This is the way, although on databricks its showing import error!\n\n# from pyspark.sql.functions import monotonicallyIncreasingId\n\n# df.select(monotonicallyIncreasingId().alias(\"rowId\"),\"*\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["# adding row num :: but first by ordering\ndf.select(row_number().over(Window.orderBy('sales')).alias('row_num'),'sales').show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-----+\nrow_num|sales|\n+-------+-----+\n      1| 1431|\n      2| 2222|\n      3| 3248|\n      4| 3317|\n      5| 3418|\n+-------+-----+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["#### Transposing || Apply function\n\n# Transposing a DF in spark in a pain!\n# Implementing apply like function(that iterates through rows/columns) is a pain! ::: This requires conversion to RDDs first"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["# storing a pyspark DF in a np array : if we want to calc rowwise maximum or Transpose etc\n\nnp.array(df.select('*').collect())[:10]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">116</span><span class=\"ansired\">]: </span>array([[&apos;A1&apos;, &apos;6805&apos;, &apos;49&apos;],\n       [&apos;A2&apos;, &apos;6110&apos;, &apos;28&apos;],\n       [&apos;A3&apos;, &apos;7888&apos;, &apos;88&apos;],\n       [&apos;A4&apos;, &apos;4773&apos;, &apos;28&apos;],\n       [&apos;A5&apos;, &apos;2222&apos;, &apos;100&apos;],\n       [&apos;A6&apos;, &apos;5489&apos;, &apos;51&apos;],\n       [&apos;A7&apos;, &apos;9951&apos;, &apos;91&apos;],\n       [&apos;A8&apos;, &apos;6673&apos;, &apos;62&apos;],\n       [&apos;A9&apos;, &apos;8987&apos;, &apos;25&apos;],\n       [&apos;A10&apos;, &apos;7057&apos;, &apos;48&apos;]], \n      dtype=&apos;&lt;U4&apos;)</div>"]}}],"execution_count":11},{"cell_type":"code","source":["#### Intersection between 2 Dataframes\n\nx = df.select('name').intersect(df.select('name')) \nx.show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+\nname|\n+----+\n A23|\n  A9|\n+----+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["#### Diference in values in 1 column WRT other column\n\nx = df[['Name']].limit(20)\n\ndf.join(x,df['name'] == x['name'],'left_anti').show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-----+------+\nName|sales|profit|\n+----+-----+------+\n A21| 4936|    21|\n A22| 3450|    45|\n A23| 4665|    58|\n A24| 3248|    34|\n A25| 3553|    36|\n+----+-----+------+\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["#### Rolling mean\nhttps://stackoverflow.com/questions/45806194/pyspark-rolling-average-using-timeseries-data"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["### Label encoding\n\nfrom pyspark.ml.feature import StringIndexer\n\n\nindexer = StringIndexer(inputCol=\"Name\", outputCol=\"index_name\")\nindexed = indexer.fit(df).transform(df)\nindexed.show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-----+------+----------+\nName|sales|profit|index_name|\n+----+-----+------+----------+\n  A1| 6805|    49|      16.0|\n  A2| 6110|    28|      14.0|\n+----+-----+------+----------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["#### obtaining duplicate rows/sunsets\n\n# +---+-----+------+----+------------+------------+\n# | ID|  ID2|Number|Name|Opening_Hour|Closing_Hour|\n# +---+-----+------+----+------------+------------+\n# |ALT|  QWA|     6|null|    08:59:00|    23:30:00|\n# |ALT|AUTRE|     2|null|    08:58:00|    23:29:00|\n# |TDR|  QWA|     3|null|    08:57:00|    23:28:00|\n# |ALT| TEST|     4|null|    08:56:00|    23:27:00|\n# |ALT|  QWA|     6|null|    08:55:00|    23:26:00|\n# |ALT|  QWA|     2|null|    08:54:00|    23:25:00|\n# |ALT|  QWA|     2|null|    08:53:00|    23:24:00|\n# +---+-----+------+----+------------+------------+\n\n\n\n# https://stackoverflow.com/questions/49559994/keep-only-duplicates-from-a-dataframe-regarding-some-field\n  \n# w = Window.partitionBy('ID', 'ID2', 'Number')\n# df.select('*', f.count('ID').over(w).alias('dupeCount'))\\\n#     .where('dupeCount > 1')\\\n#     .drop('dupeCount')\\\n#     .show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16}],"metadata":{"name":"Pandas_implement","notebookId":2615421231995778},"nbformat":4,"nbformat_minor":0}
