{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employee_id</th>\n",
       "      <th>department</th>\n",
       "      <th>region</th>\n",
       "      <th>education</th>\n",
       "      <th>gender</th>\n",
       "      <th>recruitment_channel</th>\n",
       "      <th>no_of_trainings</th>\n",
       "      <th>age</th>\n",
       "      <th>previous_year_rating</th>\n",
       "      <th>length_of_service</th>\n",
       "      <th>KPIs_met &gt;80%</th>\n",
       "      <th>awards_won?</th>\n",
       "      <th>avg_training_score</th>\n",
       "      <th>is_promoted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65438</td>\n",
       "      <td>Sales &amp; Marketing</td>\n",
       "      <td>region_7</td>\n",
       "      <td>Master's &amp; above</td>\n",
       "      <td>f</td>\n",
       "      <td>sourcing</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65141</td>\n",
       "      <td>Operations</td>\n",
       "      <td>region_22</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>m</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7513</td>\n",
       "      <td>Sales &amp; Marketing</td>\n",
       "      <td>region_19</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>m</td>\n",
       "      <td>sourcing</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2542</td>\n",
       "      <td>Sales &amp; Marketing</td>\n",
       "      <td>region_23</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>m</td>\n",
       "      <td>other</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48945</td>\n",
       "      <td>Technology</td>\n",
       "      <td>region_26</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>m</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   employee_id         department     region         education gender  \\\n",
       "0        65438  Sales & Marketing   region_7  Master's & above      f   \n",
       "1        65141         Operations  region_22        Bachelor's      m   \n",
       "2         7513  Sales & Marketing  region_19        Bachelor's      m   \n",
       "3         2542  Sales & Marketing  region_23        Bachelor's      m   \n",
       "4        48945         Technology  region_26        Bachelor's      m   \n",
       "\n",
       "  recruitment_channel  no_of_trainings  age  previous_year_rating  \\\n",
       "0            sourcing                1   35                   5.0   \n",
       "1               other                1   30                   5.0   \n",
       "2            sourcing                1   34                   3.0   \n",
       "3               other                2   39                   1.0   \n",
       "4               other                1   45                   3.0   \n",
       "\n",
       "   length_of_service  KPIs_met >80%  awards_won?  avg_training_score  \\\n",
       "0                  8              1            0                  49   \n",
       "1                  4              0            0                  60   \n",
       "2                  7              0            0                  50   \n",
       "3                 10              0            0                  50   \n",
       "4                  2              0            0                  73   \n",
       "\n",
       "   is_promoted  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train_LZdllcl.csv\")\n",
    "test = pd.read_csv(\"test_2umaH9m.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54808, 14)\n",
      "(23490, 13)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event rate in %:  8.52\n"
     ]
    }
   ],
   "source": [
    "# Learning rate\n",
    "print(\"Event rate in %: \", round((train[\"is_promoted\"].value_counts()[1] * 100)/train.shape[0], 2))\n",
    "# 8.52 - We may have to try SMOTE here. Lets see further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employee_id             54808\n",
       "department              54808\n",
       "region                  54808\n",
       "education               52399\n",
       "gender                  54808\n",
       "recruitment_channel     54808\n",
       "no_of_trainings         54808\n",
       "age                     54808\n",
       "previous_year_rating    50684\n",
       "length_of_service       54808\n",
       "KPIs_met >80%           54808\n",
       "awards_won?             54808\n",
       "avg_training_score      54808\n",
       "is_promoted             54808\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Any missing variables\n",
    "train.count()\n",
    "# education               52399               \n",
    "# previous_year_rating    50684"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bachelor's          36669\n",
      "Master's & above    14925\n",
      "Below Secondary       805\n",
      "Name: education, dtype: int64\n",
      "3.0    18618\n",
      "5.0    11741\n",
      "4.0     9877\n",
      "1.0     6223\n",
      "2.0     4225\n",
      "Name: previous_year_rating, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train['education'].value_counts())\n",
    "print(train['previous_year_rating'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = {\"previous_year_rating\":3, \"education\":\"Bachelor's\"}\n",
    "train.fillna(value=values, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical to dummies\n",
    "def convert_categorical_to_dummies(df):\n",
    "    list_to_drop = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            print(\"Converting....\", col)\n",
    "            list_to_drop.append(col)\n",
    "            df = pd.concat([df, pd.get_dummies(df[col], prefix=col, prefix_sep='_', drop_first=True)], axis=1)\n",
    "    return df, list_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employee_id</th>\n",
       "      <th>department</th>\n",
       "      <th>region</th>\n",
       "      <th>education</th>\n",
       "      <th>gender</th>\n",
       "      <th>recruitment_channel</th>\n",
       "      <th>no_of_trainings</th>\n",
       "      <th>age</th>\n",
       "      <th>previous_year_rating</th>\n",
       "      <th>length_of_service</th>\n",
       "      <th>KPIs_met &gt;80%</th>\n",
       "      <th>awards_won?</th>\n",
       "      <th>avg_training_score</th>\n",
       "      <th>is_promoted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65438</td>\n",
       "      <td>Sales &amp; Marketing</td>\n",
       "      <td>region_7</td>\n",
       "      <td>Master's &amp; above</td>\n",
       "      <td>f</td>\n",
       "      <td>sourcing</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65141</td>\n",
       "      <td>Operations</td>\n",
       "      <td>region_22</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>m</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   employee_id         department     region         education gender  \\\n",
       "0        65438  Sales & Marketing   region_7  Master's & above      f   \n",
       "1        65141         Operations  region_22        Bachelor's      m   \n",
       "\n",
       "  recruitment_channel  no_of_trainings  age  previous_year_rating  \\\n",
       "0            sourcing                1   35                   5.0   \n",
       "1               other                1   30                   5.0   \n",
       "\n",
       "   length_of_service  KPIs_met >80%  awards_won?  avg_training_score  \\\n",
       "0                  8              1            0                  49   \n",
       "1                  4              0            0                  60   \n",
       "\n",
       "   is_promoted  \n",
       "0            0  \n",
       "1            0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting.... department\n",
      "Converting.... region\n",
      "Converting.... education\n",
      "Converting.... gender\n",
      "Converting.... recruitment_channel\n",
      "Total shape of Data : (54808, 60)\n",
      "Columns which need to be dropped : ['department', 'region', 'education', 'gender', 'recruitment_channel']\n",
      "Total shape of Data : (54808, 55)\n"
     ]
    }
   ],
   "source": [
    "# Convert data to dummy variables\n",
    "processed_data, list_to_drop = convert_categorical_to_dummies(train)\n",
    "print(\"Total shape of Data :\", processed_data.shape)\n",
    "print(\"Columns which need to be dropped :\", list_to_drop)\n",
    "processed_data = processed_data.drop(list_to_drop, axis = 1)\n",
    "print(\"Total shape of Data :\", processed_data.shape)\n",
    "\n",
    "train_xs = processed_data.drop(['employee_id',\"is_promoted\"],axis=1)\n",
    "labels = train[\"is_promoted\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBOOST\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "print(xgb_model)\n",
    "\n",
    "f1_scores = cross_val_score(xgb_model, train_xs, labels, cv=3, scoring='f1')\n",
    "print(\"F1-score = \",f1_scores,\" Mean F1 score = \", np.mean(f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the models\n",
    "xgb_model.fit(train_xs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 27 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "# Create parameters to search\n",
    "params = {\n",
    "     'learning_rate': [0.01],\n",
    "     'n_estimators': [900, 1000, 1100],\n",
    "     'max_depth':[7,8,9],\n",
    "     'reg_alpha':[0.3, 0.4, 0.5]\n",
    "    }\n",
    " \n",
    "# Initializing the XGBoost Regressor\n",
    "xgb_model = xgb.XGBClassifier()\n",
    " \n",
    "# Gridsearch initializaation\n",
    "gsearch = GridSearchCV(xgb_model, params,\n",
    "                    verbose=True,\n",
    "                    cv=2,\n",
    "                    n_jobs=-1)\n",
    " \n",
    "gsearch.fit(train_xs, labels)\n",
    " \n",
    "#Printing the best chosen params\n",
    "print(\"Best Parameters :\",gsearch.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "params = {'objective':'binary:logistic', 'booster':'gbtree'}\n",
    " \n",
    "# Updating the parameter as per grid search\n",
    "params.update(gsearch.best_params_)\n",
    " \n",
    "# Initializing the XGBoost Regressor\n",
    "xgb_model = xgb.XGBClassifier(**params)\n",
    "print(xgb_model)\n",
    " \n",
    "# Cross validation scores\n",
    "f1_scores = cross_val_score(xgb_model, train_xs, labels, cv=3, scoring='f1',n_jobs=-1)\n",
    "print(\"F1_scores per fold : \",f1_scores,\" \\nMean F1_score= \", np.mean(f1_scores))\n",
    " \n",
    "# Fitting model on tuned parameters\n",
    "xgb_model.fit(train_xs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset in order to use early stopping round\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_xs, labels, test_size=0.10, stratify=labels)\n",
    "xgb_model = xgb.XGBClassifier(**params)\n",
    "\n",
    "# Training the models\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict_proba(X_test)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred[:,1])\n",
    "thresholds = np.append(thresholds, 1)\n",
    "\n",
    "f1_scores = 2*(precision*recall)/(precision+recall)\n",
    "plt.step(recall, precision, color='b', alpha=0.4, where='post')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrs = pd.DataFrame({'precision' : precision, 'recal' : recall, 'thresholds' : thresholds, 'f1_score':f1_scores})\n",
    "print(\"Threshold cutoff: \",scrs.loc[scrs['f1_score'] == scrs.f1_score.max(),'thresholds'].iloc[0])\n",
    "print(\"Max F1-score at cut-off : \",scrs.f1_score.max())\n",
    "scrs.plot(x='thresholds', y='f1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "test = pd.read_csv(\"test_2umaH9m.csv\")\n",
    "# Missing treatment\n",
    "values = {\"previous_year_rating\":3, \"education\":\"Bachelor's\"}\n",
    "test.fillna(value=values, inplace=True)\n",
    "\n",
    "# Convert data to dummy variables\n",
    "processed_data, list_to_drop = convert_categorical_to_dummies(test)\n",
    "print(\"Total shape of Data :\", processed_data.shape)\n",
    "print(\"Columns which need to be dropped :\", list_to_drop)\n",
    "processed_data = processed_data.drop(list_to_drop, axis = 1)\n",
    "print(\"Total shape of Data :\", processed_data.shape)\n",
    "\n",
    "# Removing the id attributes\n",
    "test_data = processed_data.drop(['employee_id'],axis=1)\n",
    "y_pred = xgb_model.predict_proba(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best model as of now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = (y_pred[:,1] >= scrs.f1_score.max()).astype(\"int\") # 0.47025495750708224.\n",
    "#predictions = (y_pred[:,1] >= 0.340377241373).astype(\"int\") # 0.4876760563380282 \n",
    "predictions = (y_pred[:,1] >= 0.24252495169639587).astype(\"int\") # 0.5158371040723982.\n",
    "submission_df = pd.DataFrame({'employee_id':test['employee_id'],'is_promoted':predictions})\n",
    "submission_df.to_csv(\"XGB_tuned_model_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try lightgbm\n",
    "# Initializing the LightGBM model\n",
    "gbm_model = lgb.LGBMClassifier(objective='binary')\n",
    "\n",
    "# Cross validation score \n",
    "f1_scores = cross_val_score(gbm_model, train_xs, labels, cv=5, scoring='f1',n_jobs=3)\n",
    "print(f1_scores,\" Mean = \",np.mean(f1_scores))\n",
    "\n",
    "# Splitting the dataset in order to use early stopping round\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_xs, labels, test_size=0.10, stratify=labels)\n",
    "\n",
    "# Fitting LightGBM model\n",
    "gbm_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric='auc', early_stopping_rounds=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "test = pd.read_csv(\"dataset/test.csv\")\n",
    "# Missing treatment\n",
    "values = {\"previous_year_rating\":3, \"education\":\"Bachelor's\"}\n",
    "test.fillna(value=values, inplace=True)\n",
    "\n",
    "# Convert data to dummy variables\n",
    "processed_data, list_to_drop = convert_categorical_to_dummies(test)\n",
    "print(\"Total shape of Data :\", processed_data.shape)\n",
    "print(\"Columns which need to be dropped :\", list_to_drop)\n",
    "processed_data = processed_data.drop(list_to_drop, axis = 1)\n",
    "print(\"Total shape of Data :\", processed_data.shape)\n",
    "\n",
    "# Removing the id attributes\n",
    "test_data = processed_data.drop(['employee_id'],axis=1)\n",
    "y_pred = gbm_model.predict_proba(test_data, num_iteration=gbm_model.best_iteration_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (y_pred[:,1] >= 0.24252495169639587).astype(\"int\") # 0.4996009577015163\n",
    "submission_df = pd.DataFrame({'employee_id':test['employee_id'],'is_promoted':predictions})\n",
    "submission_df.to_csv(\"lgbm_model_2.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = float(np.sum(labels == 0)) / np.sum(labels==1)\n",
    "print(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Model with solving the imbalancing problem.....No Smote as of now. Will try class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parameters to search\n",
    "params = {\n",
    "     'learning_rate': [0.01],\n",
    "     'n_estimators': [800, 900, 1000, 1100],\n",
    "     'max_depth':[5, 6, 7, 8, 9],\n",
    "     'reg_alpha':[0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "     'scale_pos_weight': [ratio]\n",
    "    }\n",
    " \n",
    "# Initializing the XGBoost Regressor\n",
    "xgb_model = xgb.XGBClassifier()\n",
    " \n",
    "# Gridsearch initializaation\n",
    "gsearch = GridSearchCV(xgb_model, params,\n",
    "                    verbose=True,\n",
    "                    cv=5,\n",
    "                    n_jobs=-1)\n",
    " \n",
    "gsearch.fit(train_xs, labels)\n",
    " \n",
    "#Printing the best chosen params\n",
    "print(\"Best Parameters :\",gsearch.best_params_)\n",
    " \n",
    "params = {'objective':'binary:logistic', 'booster':'gbtree'}\n",
    " \n",
    "# Updating the parameter as per grid search\n",
    "params.update(gsearch.best_params_)\n",
    " \n",
    "# Initializing the XGBoost Regressor\n",
    "xgb_model = xgb.XGBClassifier(**params)\n",
    "print(xgb_model)\n",
    " \n",
    "# Cross validation scores\n",
    "f1_scores = cross_val_score(xgb_model, train_xs, labels, cv=5, scoring='f1',n_jobs=2)\n",
    "print(\"F1_scores per fold : \",f1_scores,\" \\nMean F1_score= \", np.mean(f1_scores))\n",
    " \n",
    "# Fitting model on tuned parameters\n",
    "xgb_model.fit(train_xs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset in order to use early stopping round\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_xs, labels, test_size=0.10, stratify=labels)\n",
    "xgb_model = xgb.XGBClassifier(**params)\n",
    "\n",
    "# Training the models\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict_proba(X_test)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred[:,1])\n",
    "thresholds = np.append(thresholds, 1)\n",
    "\n",
    "f1_scores = 2*(precision*recall)/(precision+recall)\n",
    "plt.step(recall, precision, color='b', alpha=0.4, where='post')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrs = pd.DataFrame({'precision' : precision, 'recal' : recall, 'thresholds' : thresholds, 'f1_score':f1_scores})\n",
    "print(\"Threshold cutoff: \",scrs.loc[scrs['f1_score'] == scrs.f1_score.max(),'thresholds'].iloc[0])\n",
    "print(\"Max F1-score at cut-off : \",scrs.f1_score.max())\n",
    "scrs.plot(x='thresholds', y='f1_score')\n",
    "\n",
    "# Create submission file\n",
    "test = pd.read_csv(\"dataset/test.csv\")\n",
    "# Missing treatment\n",
    "values = {\"previous_year_rating\":3, \"education\":\"Bachelor's\"}\n",
    "test.fillna(value=values, inplace=True)\n",
    "\n",
    "# Convert data to dummy variables\n",
    "processed_data, list_to_drop = convert_categorical_to_dummies(test)\n",
    "print(\"Total shape of Data :\", processed_data.shape)\n",
    "print(\"Columns which need to be dropped :\", list_to_drop)\n",
    "processed_data = processed_data.drop(list_to_drop, axis = 1)\n",
    "print(\"Total shape of Data :\", processed_data.shape)\n",
    "\n",
    "# Removing the id attributes\n",
    "test_data = processed_data.drop(['employee_id'],axis=1)\n",
    "y_pred = xgb_model.predict_proba(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (y_pred[:,1] >= 0.7739043235778809).astype(\"int\") # 0.42322834645669294.\n",
    "submission_df = pd.DataFrame({'employee_id':test['employee_id'],'is_promoted':predictions})\n",
    "submission_df.to_csv(\"xgb_weightfactor.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Improvement. Lets try SMOTE\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n",
    "\n",
    "sm = SMOTE(random_state=2)\n",
    "X_train_res, y_train_res = sm.fit_sample(train_xs, labels.ravel())\n",
    "\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parameters to search\n",
    "params = {\n",
    "     'learning_rate': [0.01],\n",
    "     'n_estimators': [800, 900, 1000, 1100],\n",
    "     'max_depth':[5, 6, 7, 8, 9],\n",
    "     'reg_alpha':[0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "    }\n",
    " \n",
    "# Initializing the XGBoost Regressor\n",
    "xgb_model = xgb.XGBClassifier()\n",
    " \n",
    "# Gridsearch initializaation\n",
    "gsearch = GridSearchCV(xgb_model, params,\n",
    "                    verbose=True,\n",
    "                    cv=5,\n",
    "                    n_jobs=-1)\n",
    " \n",
    "gsearch.fit(train_xs, labels)\n",
    " \n",
    "#Printing the best chosen params\n",
    "print(\"Best Parameters :\",gsearch.best_params_)\n",
    " \n",
    "params = {'objective':'binary:logistic', 'booster':'gbtree'}\n",
    " \n",
    "# Updating the parameter as per grid search\n",
    "params.update(gsearch.best_params_)\n",
    " \n",
    "# Initializing the XGBoost Regressor\n",
    "xgb_model = xgb.XGBClassifier(**params)\n",
    "print(xgb_model)\n",
    " \n",
    "# Cross validation scores\n",
    "f1_scores = cross_val_score(xgb_model, train_xs, labels, cv=5, scoring='f1',n_jobs=2)\n",
    "print(\"F1_scores per fold : \",f1_scores,\" \\nMean F1_score= \", np.mean(f1_scores))\n",
    " \n",
    "# Fitting model on tuned parameters\n",
    "xgb_model.fit(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset in order to use early stopping round\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_xs, labels, test_size=0.10, stratify=labels)\n",
    "xgb_model = xgb.XGBClassifier(**params)\n",
    "\n",
    "# Training the models\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict_proba(X_test)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred[:,1])\n",
    "thresholds = np.append(thresholds, 1)\n",
    "\n",
    "f1_scores = 2*(precision*recall)/(precision+recall)\n",
    "plt.step(recall, precision, color='b', alpha=0.4, where='post')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrs = pd.DataFrame({'precision' : precision, 'recal' : recall, 'thresholds' : thresholds, 'f1_score':f1_scores})\n",
    "print(\"Threshold cutoff: \",scrs.loc[scrs['f1_score'] == scrs.f1_score.max(),'thresholds'].iloc[0])\n",
    "print(\"Max F1-score at cut-off : \",scrs.f1_score.max())\n",
    "scrs.plot(x='thresholds', y='f1_score')\n",
    "\n",
    "# Create submission file\n",
    "test = pd.read_csv(\"dataset/test.csv\")\n",
    "# Missing treatment\n",
    "values = {\"previous_year_rating\":3, \"education\":\"Bachelor's\"}\n",
    "test.fillna(value=values, inplace=True)\n",
    "\n",
    "# Convert data to dummy variables\n",
    "processed_data, list_to_drop = convert_categorical_to_dummies(test)\n",
    "print(\"Total shape of Data :\", processed_data.shape)\n",
    "print(\"Columns which need to be dropped :\", list_to_drop)\n",
    "processed_data = processed_data.drop(list_to_drop, axis = 1)\n",
    "print(\"Total shape of Data :\", processed_data.shape)\n",
    "\n",
    "# Removing the id attributes\n",
    "test_data = processed_data.drop(['employee_id'],axis=1)\n",
    "y_pred = xgb_model.predict_proba(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (y_pred[:,1] >= 0.30655303597450256).astype(\"int\") # 0.4995722840034217.\n",
    "submission_df = pd.DataFrame({'employee_id':test['employee_id'],'is_promoted':predictions})\n",
    "submission_df.to_csv(\"smote_xgb_weightfactor.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not improving at all. Best solution comes with XGBoost. Will now try to create new features.\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"dataset/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = {\"previous_year_rating\":3, \"education\":\"Bachelor's\"}\n",
    "train.fillna(value=values, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dep_mean_score = round(train.groupby([\"department\"], as_index=False)[\"avg_training_score\"].mean())\n",
    "#reg_mean_score = round(train.groupby([\"region\"], as_index=False)[\"avg_training_score\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 1 - It make more senese to promote deparmment wise if average score is higher.\n",
    "dep_mean_score = round(train.groupby([\"department\"], as_index=False)[\"avg_training_score\"].mean())\n",
    "train = pd.merge(train, dep_mean_score, on = \"department\", how='inner',suffixes=(\"\", \"_bydep\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 2 - average_score_by region\n",
    "reg_mean_score = round(train.groupby([\"region\"], as_index=False)[\"avg_training_score\"].mean())\n",
    "train = pd.merge(train, reg_mean_score, on = \"region\", how='inner',suffixes=(\"\", \"_byregion\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 3 & 4 avg training score is high\n",
    "train[\"avg_training_score_bydep_high\"] = (train[\"avg_training_score_bydep\"] <= train[\"avg_training_score\"]).astype(int)\n",
    "train[\"avg_training_score_byregion_high\"] = (train[\"avg_training_score_byregion\"] <= train[\"avg_training_score\"]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to dummy variables\n",
    "processed_data, list_to_drop = convert_categorical_to_dummies(train)\n",
    "print(\"Total shape of Data :\", processed_data.shape)\n",
    "print(\"Columns which need to be dropped :\", list_to_drop)\n",
    "processed_data = processed_data.drop(list_to_drop, axis = 1)\n",
    "print(\"Total shape of Data :\", processed_data.shape)\n",
    "\n",
    "train_xs = processed_data.drop(['employee_id',\"is_promoted\"],axis=1)\n",
    "labels = train[\"is_promoted\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parameters to search\n",
    "params = {\n",
    "     'learning_rate': [0.01],\n",
    "     'n_estimators': [900, 1000, 1100],\n",
    "     'max_depth':[7,8,9],\n",
    "     'reg_alpha':[0.3, 0.4, 0.5]\n",
    "    }\n",
    " \n",
    "# Initializing the XGBoost Regressor\n",
    "xgb_model = xgb.XGBClassifier()\n",
    " \n",
    "# Gridsearch initializaation\n",
    "gsearch = GridSearchCV(xgb_model, params,\n",
    "                    verbose=True,\n",
    "                    cv=5,\n",
    "                    n_jobs=2)\n",
    " \n",
    "gsearch.fit(train_xs, labels)\n",
    " \n",
    "#Printing the best chosen params\n",
    "print(\"Best Parameters :\",gsearch.best_params_)\n",
    " \n",
    "params = {'objective':'binary:logistic', 'booster':'gbtree'}\n",
    " \n",
    "# Updating the parameter as per grid search\n",
    "params.update(gsearch.best_params_)\n",
    " \n",
    "# Initializing the XGBoost Regressor\n",
    "xgb_model = xgb.XGBClassifier(**params)\n",
    "print(xgb_model)\n",
    " \n",
    "# Cross validation scores\n",
    "f1_scores = cross_val_score(xgb_model, train_xs, labels, cv=5, scoring='f1',n_jobs=2)\n",
    "print(\"F1_scores per fold : \",f1_scores,\" \\nMean F1_score= \", np.mean(f1_scores))\n",
    " \n",
    "# Fitting model on tuned parameters\n",
    "xgb_model.fit(train_xs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset in order to use early stopping round\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_xs, labels, test_size=0.10, stratify=labels)\n",
    "xgb_model = xgb.XGBClassifier(**params)\n",
    "\n",
    "# Training the models\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict_proba(X_test)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred[:,1])\n",
    "thresholds = np.append(thresholds, 1)\n",
    "\n",
    "f1_scores = 2*(precision*recall)/(precision+recall)\n",
    "plt.step(recall, precision, color='b', alpha=0.4, where='post')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrs = pd.DataFrame({'precision' : precision, 'recal' : recall, 'thresholds' : thresholds, 'f1_score':f1_scores})\n",
    "print(\"Threshold cutoff: \",scrs.loc[scrs['f1_score'] == scrs.f1_score.max(),'thresholds'].iloc[0])\n",
    "print(\"Max F1-score at cut-off : \",scrs.f1_score.max())\n",
    "scrs.plot(x='thresholds', y='f1_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "test = pd.read_csv(\"dataset/test.csv\")\n",
    "# Missing treatment\n",
    "values = {\"previous_year_rating\":3, \"education\":\"Bachelor's\"}\n",
    "test.fillna(value=values, inplace=True)\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 1 & 2 - average_score_by region\n",
    "test = pd.merge(test, dep_mean_score, on = \"department\", how='inner',suffixes=(\"\", \"_bydep\"))\n",
    "test = pd.merge(test, reg_mean_score, on = \"region\", how='inner',suffixes=(\"\", \"_byregion\"))\n",
    "\n",
    "# Feature 3 & 4 avg training score is high\n",
    "test[\"avg_training_score_bydep_high\"] = (test[\"avg_training_score_bydep\"] <= test[\"avg_training_score\"]).astype(int)\n",
    "test[\"avg_training_score_byregion_high\"] = (test[\"avg_training_score_byregion\"] <= test[\"avg_training_score\"]).astype(int)\n",
    "\n",
    "# Convert data to dummy variables\n",
    "processed_data, list_to_drop = convert_categorical_to_dummies(test)\n",
    "print(\"Total shape of Data :\", processed_data.shape)\n",
    "print(\"Columns which need to be dropped :\", list_to_drop)\n",
    "processed_data = processed_data.drop(list_to_drop, axis = 1)\n",
    "print(\"Total shape of Data :\", processed_data.shape)\n",
    "\n",
    "# Removing the id attributes\n",
    "test_data = processed_data.drop(['employee_id'],axis=1)\n",
    "y_pred = xgb_model.predict_proba(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (y_pred[:,1] >= 0.29761573672294617).astype(\"int\") # 0.5050675675675677.\n",
    "submission_df = pd.DataFrame({'employee_id':test['employee_id'],'is_promoted':predictions})\n",
    "submission_df.to_csv(\"feature_xgb_weightfactor.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
